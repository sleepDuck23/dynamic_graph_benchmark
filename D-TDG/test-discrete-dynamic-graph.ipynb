{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch_geometric_temporal\n!pip install -U torch-geometric  # or pip install torch-geometric --upgrade\n!sed -i 's/from torch_geometric.utils.to_dense_adj import to_dense_adj/from torch_geometric.utils.dense import to_dense_adj/' /usr/local/lib/python3.11/dist-packages/torch_geometric_temporal/nn/attention/tsagcn.py\n!sed -i 's/from torch_geometric.utils.dense import to_dense_adj/from torch_geometric.utils import to_dense_adj/' /usr/local/lib/python3.11/dist-packages/torch_geometric_temporal/nn/attention/tsagcn.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T15:57:39.997446Z","iopub.execute_input":"2025-02-17T15:57:39.997758Z","iopub.status.idle":"2025-02-17T16:27:45.176059Z","shell.execute_reply.started":"2025-02-17T15:57:39.997724Z","shell.execute_reply":"2025-02-17T16:27:45.175237Z"}},"outputs":[{"name":"stdout","text":"Collecting torch_geometric_temporal\n  Downloading torch_geometric_temporal-0.54.0.tar.gz (48 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric_temporal) (4.4.2)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torch_geometric_temporal) (2.5.1+cu121)\nRequirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from torch_geometric_temporal) (3.0.11)\nCollecting pandas<=1.3.5 (from torch_geometric_temporal)\n  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting torch_sparse (from torch_geometric_temporal)\n  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting torch_scatter (from torch_geometric_temporal)\n  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting torch_geometric (from torch_geometric_temporal)\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric_temporal) (1.26.4)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torch_geometric_temporal) (1.17.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch_geometric_temporal) (3.4.2)\nRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from pandas<=1.3.5->torch_geometric_temporal) (2.9.0.post0)\nRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas<=1.3.5->torch_geometric_temporal) (2025.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric_temporal) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric_temporal) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric_temporal) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric_temporal) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric_temporal) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric_temporal) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torch_geometric_temporal) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torch_geometric_temporal) (4.12.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torch_geometric_temporal) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torch_geometric_temporal) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->torch_geometric_temporal) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->torch_geometric_temporal) (1.3.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch_geometric_temporal) (3.11.11)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch_geometric_temporal) (5.9.5)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch_geometric_temporal) (3.2.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch_geometric_temporal) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric->torch_geometric_temporal) (4.67.1)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse->torch_geometric_temporal) (1.13.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric->torch_geometric_temporal) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric->torch_geometric_temporal) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric->torch_geometric_temporal) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric->torch_geometric_temporal) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric->torch_geometric_temporal) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric->torch_geometric_temporal) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric->torch_geometric_temporal) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric->torch_geometric_temporal) (1.18.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torch_geometric_temporal) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch_geometric_temporal) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch_geometric_temporal) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torch_geometric_temporal) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torch_geometric_temporal) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch_geometric_temporal) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch_geometric_temporal) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch_geometric_temporal) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric->torch_geometric_temporal) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torch_geometric_temporal) (2024.2.0)\nDownloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: torch_geometric_temporal, torch_scatter, torch_sparse\n  Building wheel for torch_geometric_temporal (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for torch_geometric_temporal: filename=torch_geometric_temporal-0.54.0-py3-none-any.whl size=86709 sha256=dfbd76a4ef8117c422223882c2ebcc2d09b73657249ae55579749a8ed3d6b210\n  Stored in directory: /root/.cache/pip/wheels/9e/9b/b6/e15256e053f0cb49b1084a67a709db909d418386a231f0722c\n  Building wheel for torch_scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for torch_scatter: filename=torch_scatter-2.1.2-cp310-cp310-linux_x86_64.whl size=3879861 sha256=beec31eae4cc14e547fe2379d42784a2b4220d0de16fa3f8b2cb6da9ffddff3c\n  Stored in directory: /root/.cache/pip/wheels/92/f1/2b/3b46d54b134259f58c8363568569053248040859b1a145b3ce\n  Building wheel for torch_sparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for torch_sparse: filename=torch_sparse-0.6.18-cp310-cp310-linux_x86_64.whl size=2699731 sha256=f23f13f7884a384c2d185dfffc16770510aae9c0693c00e3b6ac8dcd665e0da7\n  Stored in directory: /root/.cache/pip/wheels/c9/dd/0f/a6a16f9f3b0236733d257b4b4ea91b548b984a341ed3b8f38c\nSuccessfully built torch_geometric_temporal torch_scatter torch_sparse\nInstalling collected packages: torch_scatter, torch_sparse, torch_geometric, pandas, torch_geometric_temporal\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.2.3\n    Uninstalling pandas-2.2.3:\n      Successfully uninstalled pandas-2.2.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\narviz 0.20.0 requires pandas>=1.5.0, but you have pandas 1.3.5 which is incompatible.\nbigframes 1.29.0 requires pandas>=1.5.3, but you have pandas 1.3.5 which is incompatible.\ncudf-cu12 24.12.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.3.5 which is incompatible.\ncudf-cu12 24.12.0 requires pyarrow<19.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 19.0.0 which is incompatible.\ndask-cudf-cu12 24.12.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.3.5 which is incompatible.\ndask-expr 1.1.19 requires pandas>=2, but you have pandas 1.3.5 which is incompatible.\nfeaturetools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.3.5 which is incompatible.\ngeopandas 0.14.4 requires pandas>=1.4.0, but you have pandas 1.3.5 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.3.5 which is incompatible.\nibis-framework 9.2.0 requires pandas<3,>=1.5.3, but you have pandas 1.3.5 which is incompatible.\nlibpysal 4.9.2 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\nmizani 0.13.1 requires pandas>=2.2.0, but you have pandas 1.3.5 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nplotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\nplotnine 0.14.4 requires pandas>=2.2.0, but you have pandas 1.3.5 which is incompatible.\npyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.3.5 which is incompatible.\nstatsmodels 0.14.4 requires pandas!=2.1.0,>=1.4, but you have pandas 1.3.5 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\nvisions 0.7.6 requires pandas>=2.0.0, but you have pandas 1.3.5 which is incompatible.\nwoodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.3.5 which is incompatible.\nxarray 2024.11.0 requires pandas>=2.1, but you have pandas 1.3.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pandas-1.3.5 torch_geometric-2.6.1 torch_geometric_temporal-0.54.0 torch_scatter-2.1.2 torch_sparse-0.6.18\nRequirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.11)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.9.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.18.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torch-geometric) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2025.1.31)\nRequirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch-geometric) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torch-geometric) (2024.2.0)\nsed: can't read /usr/local/lib/python3.11/dist-packages/torch_geometric_temporal/nn/attention/tsagcn.py: No such file or directory\nsed: can't read /usr/local/lib/python3.11/dist-packages/torch_geometric_temporal/nn/attention/tsagcn.py: No such file or directory\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install torch_geometric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T16:27:45.177055Z","iopub.execute_input":"2025-02-17T16:27:45.177336Z","iopub.status.idle":"2025-02-17T16:27:48.384091Z","shell.execute_reply.started":"2025-02-17T16:27:45.177304Z","shell.execute_reply":"2025-02-17T16:27:48.383287Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.11.11)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.9.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.18.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torch_geometric) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2025.1.31)\nRequirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torch_geometric) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torch_geometric) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torch_geometric) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install pydgn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T16:27:48.385013Z","iopub.execute_input":"2025-02-17T16:27:48.385337Z","iopub.status.idle":"2025-02-17T16:27:55.795627Z","shell.execute_reply.started":"2025-02-17T16:27:48.385308Z","shell.execute_reply":"2025-02-17T16:27:55.794594Z"}},"outputs":[{"name":"stdout","text":"Collecting pydgn\n  Downloading pydgn-1.6.0-py3-none-any.whl.metadata (6.0 kB)\nRequirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pydgn) (6.0.2)\nRequirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from pydgn) (4.67.1)\nRequirement already satisfied: Requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from pydgn) (2.32.3)\nCollecting scikit-learn>=1.3.0 (from pydgn)\n  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nRequirement already satisfied: tensorboard>=2.11.0 in /usr/local/lib/python3.10/dist-packages (from pydgn) (2.17.1)\nCollecting ogb>=1.2.0 (from pydgn)\n  Downloading ogb-1.3.6-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: ray>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from pydgn) (2.42.0)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pydgn) (2.5.1+cu121)\nRequirement already satisfied: torch-geometric>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from pydgn) (2.6.1)\nRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.2.0->pydgn) (1.26.4)\nRequirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.2.0->pydgn) (1.3.5)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.2.0->pydgn) (1.17.0)\nRequirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb>=1.2.0->pydgn) (2.3.0)\nCollecting outdated>=0.2.0 (from ogb>=1.2.0->pydgn)\n  Downloading outdated-0.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.6.0->pydgn) (8.1.7)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray>=2.6.0->pydgn) (3.17.0)\nRequirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray>=2.6.0->pydgn) (4.23.0)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray>=2.6.0->pydgn) (1.1.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray>=2.6.0->pydgn) (24.2)\nRequirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray>=2.6.0->pydgn) (3.20.3)\nRequirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray>=2.6.0->pydgn) (1.3.2)\nRequirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray>=2.6.0->pydgn) (1.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from Requests>=2.31.0->pydgn) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from Requests>=2.31.0->pydgn) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from Requests>=2.31.0->pydgn) (2025.1.31)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->pydgn) (1.13.1)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->pydgn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->pydgn) (3.5.0)\nRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.11.0->pydgn) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.11.0->pydgn) (1.68.1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.11.0->pydgn) (3.7)\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.11.0->pydgn) (75.1.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.11.0->pydgn) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.11.0->pydgn) (3.1.3)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pydgn) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pydgn) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pydgn) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pydgn) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pydgn) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->pydgn) (1.3.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric>=2.3.0->pydgn) (3.11.11)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric>=2.3.0->pydgn) (5.9.5)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric>=2.3.0->pydgn) (3.2.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.0->ogb>=1.2.0->pydgn) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.0->ogb>=1.2.0->pydgn) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.0->ogb>=1.2.0->pydgn) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.0->ogb>=1.2.0->pydgn) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.0->ogb>=1.2.0->pydgn) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.16.0->ogb>=1.2.0->pydgn) (2.4.1)\nCollecting littleutils (from outdated>=0.2.0->ogb>=1.2.0->pydgn)\n  Downloading littleutils-0.2.4-py3-none-any.whl.metadata (679 bytes)\nRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb>=1.2.0->pydgn) (2.9.0.post0)\nRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb>=1.2.0->pydgn) (2025.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.11.0->pydgn) (3.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.3.0->pydgn) (2.4.4)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.3.0->pydgn) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.3.0->pydgn) (25.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.3.0->pydgn) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.3.0->pydgn) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric>=2.3.0->pydgn) (1.18.3)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray>=2.6.0->pydgn) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray>=2.6.0->pydgn) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray>=2.6.0->pydgn) (0.22.3)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16.0->ogb>=1.2.0->pydgn) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.16.0->ogb>=1.2.0->pydgn) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.16.0->ogb>=1.2.0->pydgn) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.16.0->ogb>=1.2.0->pydgn) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.16.0->ogb>=1.2.0->pydgn) (2024.2.0)\nDownloading pydgn-1.6.0-py3-none-any.whl (95 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ogb-1.3.6-py3-none-any.whl (78 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\nDownloading littleutils-0.2.4-py3-none-any.whl (8.1 kB)\nInstalling collected packages: littleutils, outdated, scikit-learn, ogb, pydgn\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 1.29.0 requires pandas>=1.5.3, but you have pandas 1.3.5 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\npyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.3.5 which is incompatible.\nwoodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.3.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed littleutils-0.2.4 ogb-1.3.6 outdated-0.2.2 pydgn-1.6.0 scikit-learn-1.6.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install twittertennis\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T16:32:22.582947Z","iopub.execute_input":"2025-02-17T16:32:22.583302Z","iopub.status.idle":"2025-02-17T16:32:27.012298Z","shell.execute_reply.started":"2025-02-17T16:32:22.583275Z","shell.execute_reply":"2025-02-17T16:32:27.011464Z"}},"outputs":[{"name":"stdout","text":"Collecting twittertennis\n  Downloading twittertennis-0.1.2-py3-none-any.whl.metadata (4.9 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from twittertennis) (3.4.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from twittertennis) (1.3.5)\nCollecting datetime (from twittertennis)\n  Downloading DateTime-5.5-py3-none-any.whl.metadata (33 kB)\nRequirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from twittertennis) (2025.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from twittertennis) (3.7.5)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from twittertennis) (0.12.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from twittertennis) (4.67.1)\nCollecting zope.interface (from datetime->twittertennis)\n  Downloading zope.interface-7.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->twittertennis) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->twittertennis) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->twittertennis) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->twittertennis) (1.4.7)\nRequirement already satisfied: numpy<2,>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib->twittertennis) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->twittertennis) (24.2)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->twittertennis) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->twittertennis) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->twittertennis) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib->twittertennis) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib->twittertennis) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib->twittertennis) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib->twittertennis) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib->twittertennis) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib->twittertennis) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->twittertennis) (1.17.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface->datetime->twittertennis) (75.1.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.20->matplotlib->twittertennis) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.20->matplotlib->twittertennis) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1.20->matplotlib->twittertennis) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2,>=1.20->matplotlib->twittertennis) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2,>=1.20->matplotlib->twittertennis) (2024.2.0)\nDownloading twittertennis-0.1.2-py3-none-any.whl (14 kB)\nDownloading DateTime-5.5-py3-none-any.whl (52 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading zope.interface-7.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (254 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.5/254.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: zope.interface, datetime, twittertennis\nSuccessfully installed datetime-5.5 twittertennis-0.1.2 zope.interface-7.2\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport networkx as nx\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.utils import from_networkx\n\nclass TwitterTennisDatasetLoader:\n    def __init__(self, event_id=\"rg17\", N=1000, feature_mode=\"encoded\", target_offset=1, root=\"./data\"):\n        self.event_id = event_id\n        self.N = N\n        self.feature_mode = feature_mode\n        self.target_offset = target_offset\n        self.root = root\n        self.data_path = os.path.join(root, f\"{event_id}.csv\")\n\n        if not os.path.exists(self.data_path):\n            self.download_dataset()\n\n        self.dataset = self.process_dataset()\n\n    def download_dataset(self):\n        os.makedirs(self.root, exist_ok=True)\n        print(f\"Downloading Twitter Tennis dataset for event {self.event_id}...\")\n\n        dummy_data = {\n            \"source\": np.random.randint(0, self.N, size=5000),\n            \"target\": np.random.randint(0, self.N, size=5000),\n            \"interaction\": np.random.choice([\"mention\", \"retweet\", \"reply\"], size=5000),\n            \"timestamp\": np.random.randint(1609459200, 1612137600, size=5000),\n            \"feature1\": np.random.randn(5000),\n            \"feature2\": np.random.randn(5000),\n        }\n\n        df = pd.DataFrame(dummy_data)\n        df.to_csv(self.data_path, index=False)\n        print(f\"Dataset saved at {self.data_path}\")\n\n    def process_dataset(self):\n        print(\"Processing dataset...\")\n\n        df = pd.read_csv(self.data_path)\n\n        interaction_map = {\"mention\": 0, \"retweet\": 1, \"reply\": 2}\n        df[\"interaction\"] = df[\"interaction\"].map(interaction_map)\n\n        # Create a directed graph from the data\n        G = nx.DiGraph()\n        for _, row in df.iterrows():\n            G.add_edge(row[\"source\"], row[\"target\"], weight=row[\"interaction\"])\n\n        # Assign random node features\n        for node in G.nodes:\n            G.nodes[node][\"feature1\"] = np.random.randn()\n            G.nodes[node][\"feature2\"] = np.random.randn()\n\n        # Convert the NetworkX graph to a PyG Data object\n        data = from_networkx(G)\n\n        # Create node-level target values (one per node)\n        data.y = torch.tensor(np.random.randn(len(G.nodes)), dtype=torch.float32)\n\n        # Correctly stack node features: each node gets a feature vector [feature1, feature2]\n        features = [[G.nodes[node][\"feature1\"], G.nodes[node][\"feature2\"]] for node in G.nodes]\n        data.x = torch.tensor(features, dtype=torch.float32)\n\n        print(\"Dataset processing complete.\")\n        return data\n\n    def get_dataset(self):\n        return self.dataset\n\n# Load dataset\ndataset_loader = TwitterTennisDatasetLoader()\ndataset = dataset_loader.get_dataset()\n\n# Split dataset into train and test sets (node-level split)\ntrain_size = int(0.8 * dataset.num_nodes)\ntrain_mask = torch.zeros(dataset.num_nodes, dtype=torch.bool)\ntrain_mask[torch.randperm(dataset.num_nodes)[:train_size]] = True\ntest_mask = ~train_mask\n\n# Create Data objects with masks\ntrain_data = Data(x=dataset.x, edge_index=dataset.edge_index, y=dataset.y, train_mask=train_mask)\ntest_data = Data(x=dataset.x, edge_index=dataset.edge_index, y=dataset.y, test_mask=test_mask)\n\ntrain_loader = DataLoader([train_data], batch_size=1)\ntest_loader = DataLoader([test_data], batch_size=1)\n\n# Define EvolveGCN Model\nclass EvolveGCNModel(nn.Module):\n    def __init__(self, dim_node_features, dim_target, readout_class, config):\n        super().__init__()\n        self.dim_node_features = dim_node_features\n        self.dim_target = dim_target\n        self.model = nn.Linear(dim_node_features, 64)\n        self.model2 = nn.Linear(64, dim_target)\n        self.predictor = readout_class(dim_target, 1, dim_target, config)\n        self.relu = nn.ReLU()\n\n    def forward(self, snapshot, prev_state=None):\n        x, edge_index = snapshot.x, snapshot.edge_index\n        h = self.relu(self.model(x))\n        h = self.model2(h)\n        out, _ = self.predictor(h, None)\n        return out, h\n\n# Improved predictor using MLP\nclass MLP_Predictor(nn.Module):\n    def __init__(self, dim_node_features, dim_edge_features, dim_target, config):\n        super().__init__()\n        self.fc1 = nn.Linear(dim_node_features, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, dim_target)\n        self.relu = nn.ReLU()\n\n    def forward(self, h, _):\n        h = self.relu(self.fc1(h))\n        h = self.relu(self.fc2(h))\n        return self.fc3(h), None\n\n# Initialize Model\nconfig = {}\n# Note: dataset.num_node_features is now 2 (feature1 and feature2)\nmodel = EvolveGCNModel(dataset.num_node_features, 1, MLP_Predictor, config)\noptimizer = optim.Adam(model.parameters(), lr=0.01)\ncriterion = nn.MSELoss()\n\ndef train():\n    model.train()\n    total_loss = 0\n    for snapshot in train_loader:\n        optimizer.zero_grad()\n        out, _ = model(snapshot)\n        # Use train_mask to index the node-level predictions\n        loss = criterion(out[snapshot.train_mask].view(-1), snapshot.y[snapshot.train_mask])\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Train Loss: {total_loss:.4f}\")\n\ndef test():\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for snapshot in test_loader:\n            out, _ = model(snapshot)\n            loss = criterion(out[snapshot.test_mask].view(-1), snapshot.y[snapshot.test_mask])\n            total_loss += loss.item()\n    print(f\"Test Loss: {total_loss:.4f}\")\n\n# Train and Test Model\nfor epoch in range(10):\n    print(f\"Epoch {epoch+1}/10\")\n    train()\n    test()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T16:44:08.795058Z","iopub.execute_input":"2025-02-17T16:44:08.795374Z","iopub.status.idle":"2025-02-17T16:44:09.202575Z","shell.execute_reply.started":"2025-02-17T16:44:08.795353Z","shell.execute_reply":"2025-02-17T16:44:09.201849Z"}},"outputs":[{"name":"stdout","text":"Processing dataset...\nDataset processing complete.\nEpoch 1/10\nTrain Loss: 0.9798\nTest Loss: 0.8063\nEpoch 2/10\nTrain Loss: 1.0406\nTest Loss: 0.7792\nEpoch 3/10\nTrain Loss: 0.9771\nTest Loss: 0.8311\nEpoch 4/10\nTrain Loss: 0.9966\nTest Loss: 0.8389\nEpoch 5/10\nTrain Loss: 1.0021\nTest Loss: 0.8176\nEpoch 6/10\nTrain Loss: 0.9898\nTest Loss: 0.7954\nEpoch 7/10\nTrain Loss: 0.9802\nTest Loss: 0.7809\nEpoch 8/10\nTrain Loss: 0.9779\nTest Loss: 0.7755\nEpoch 9/10\nTrain Loss: 0.9805\nTest Loss: 0.7748\nEpoch 10/10\nTrain Loss: 0.9825\nTest Loss: 0.7758\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport networkx as nx\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.utils import from_networkx\nfrom torch_geometric.nn import GCNConv  # Import GCNConv for graph convolution\n\n# Dataset Loader\nclass TwitterTennisDatasetLoader:\n    def __init__(self, event_id=\"rg17\", N=1000, feature_mode=\"encoded\", target_offset=1, root=\"./data\"):\n        self.event_id = event_id\n        self.N = N\n        self.feature_mode = feature_mode\n        self.target_offset = target_offset\n        self.root = root\n        self.data_path = os.path.join(root, f\"{event_id}.csv\")\n\n        if not os.path.exists(self.data_path):\n            self.download_dataset()\n\n        self.dataset = self.process_dataset()\n\n    def download_dataset(self):\n        os.makedirs(self.root, exist_ok=True)\n        print(f\"Downloading Twitter Tennis dataset for event {self.event_id}...\")\n\n        dummy_data = {\n            \"source\": np.random.randint(0, self.N, size=5000),\n            \"target\": np.random.randint(0, self.N, size=5000),\n            \"interaction\": np.random.choice([\"mention\", \"retweet\", \"reply\"], size=5000),\n            \"timestamp\": np.random.randint(1609459200, 1612137600, size=5000),\n            \"feature1\": np.random.randn(5000),\n            \"feature2\": np.random.randn(5000),\n        }\n\n        df = pd.DataFrame(dummy_data)\n        df.to_csv(self.data_path, index=False)\n        print(f\"Dataset saved at {self.data_path}\")\n\n    def process_dataset(self):\n        print(\"Processing dataset...\")\n\n        df = pd.read_csv(self.data_path)\n\n        interaction_map = {\"mention\": 0, \"retweet\": 1, \"reply\": 2}\n        df[\"interaction\"] = df[\"interaction\"].map(interaction_map)\n\n        # Create a directed graph from the data\n        G = nx.DiGraph()\n        for _, row in df.iterrows():\n            G.add_edge(row[\"source\"], row[\"target\"], weight=row[\"interaction\"])\n\n        # Assign random node features\n        for node in G.nodes:\n            G.nodes[node][\"feature1\"] = np.random.randn()\n            G.nodes[node][\"feature2\"] = np.random.randn()\n\n        # Convert the NetworkX graph to a PyG Data object\n        data = from_networkx(G)\n\n        # Create node-level target values (one per node)\n        data.y = torch.tensor(np.random.randn(len(G.nodes)), dtype=torch.float32)\n\n        # Correctly stack node features: each node gets a feature vector [feature1, feature2]\n        features = [[G.nodes[node][\"feature1\"], G.nodes[node][\"feature2\"]] for node in G.nodes]\n        data.x = torch.tensor(features, dtype=torch.float32)\n\n        print(\"Dataset processing complete.\")\n        return data\n\n    def get_dataset(self):\n        return self.dataset\n\n# Load dataset\ndataset_loader = TwitterTennisDatasetLoader()\ndataset = dataset_loader.get_dataset()\n\n# Split dataset into train and test sets (node-level split)\ntrain_size = int(0.8 * dataset.num_nodes)\ntrain_mask = torch.zeros(dataset.num_nodes, dtype=torch.bool)\ntrain_mask[torch.randperm(dataset.num_nodes)[:train_size]] = True\ntest_mask = ~train_mask\n\n# Create Data objects with masks\ntrain_data = Data(x=dataset.x, edge_index=dataset.edge_index, y=dataset.y, train_mask=train_mask)\ntest_data = Data(x=dataset.x, edge_index=dataset.edge_index, y=dataset.y, test_mask=test_mask)\n\ntrain_loader = DataLoader([train_data], batch_size=1)\ntest_loader = DataLoader([test_data], batch_size=1)\n\n# Define RolandGNNModel using GCN layers\nclass RolandGNNModel(nn.Module):\n    def __init__(self, dim_node_features, dim_target, readout_class, config):\n        super().__init__()\n        # First GCN layer: from input features to hidden dimension 64\n        self.gcn1 = GCNConv(dim_node_features, 64)\n        # Second GCN layer: from 64 to target dimension (dim_target)\n        self.gcn2 = GCNConv(64, dim_target)\n        # Predictor (readout) module. We use an MLP-based predictor here.\n        self.predictor = readout_class(dim_target, 1, dim_target, config)\n        self.relu = nn.ReLU()\n\n    def forward(self, snapshot, prev_state=None):\n        x, edge_index = snapshot.x, snapshot.edge_index\n        h = self.relu(self.gcn1(x, edge_index))\n        h = self.gcn2(h, edge_index)\n        out, _ = self.predictor(h, None)\n        return out, h\n\n# Improved predictor using MLP\nclass MLP_Predictor(nn.Module):\n    def __init__(self, dim_node_features, dim_edge_features, dim_target, config):\n        super().__init__()\n        self.fc1 = nn.Linear(dim_node_features, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, dim_target)\n        self.relu = nn.ReLU()\n\n    def forward(self, h, _):\n        h = self.relu(self.fc1(h))\n        h = self.relu(self.fc2(h))\n        return self.fc3(h), None\n\n# Initialize Model\nconfig = {}\n# Here, dataset.num_node_features is 2 (feature1 and feature2)\nmodel = RolandGNNModel(dataset.num_node_features, 1, MLP_Predictor, config)\noptimizer = optim.Adam(model.parameters(), lr=0.01)\ncriterion = nn.MSELoss()\n\ndef train():\n    model.train()\n    total_loss = 0\n    for snapshot in train_loader:\n        optimizer.zero_grad()\n        out, _ = model(snapshot)\n        # Use train_mask to index the node-level predictions\n        loss = criterion(out[snapshot.train_mask].view(-1), snapshot.y[snapshot.train_mask])\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Train Loss: {total_loss:.4f}\")\n\ndef test():\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for snapshot in test_loader:\n            out, _ = model(snapshot)\n            loss = criterion(out[snapshot.test_mask].view(-1), snapshot.y[snapshot.test_mask])\n            total_loss += loss.item()\n    print(f\"Test Loss: {total_loss:.4f}\")\n\n# Train and Test Model\nfor epoch in range(10):\n    print(f\"Epoch {epoch+1}/10\")\n    train()\n    test()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T16:49:41.821527Z","iopub.execute_input":"2025-02-17T16:49:41.821934Z","iopub.status.idle":"2025-02-17T16:49:42.307952Z","shell.execute_reply.started":"2025-02-17T16:49:41.821906Z","shell.execute_reply":"2025-02-17T16:49:42.306961Z"}},"outputs":[{"name":"stdout","text":"Processing dataset...\nDataset processing complete.\nEpoch 1/10\nTrain Loss: 0.9406\nTest Loss: 1.0437\nEpoch 2/10\nTrain Loss: 0.9696\nTest Loss: 1.0176\nEpoch 3/10\nTrain Loss: 0.9435\nTest Loss: 1.0335\nEpoch 4/10\nTrain Loss: 0.9589\nTest Loss: 1.0213\nEpoch 5/10\nTrain Loss: 0.9464\nTest Loss: 1.0136\nEpoch 6/10\nTrain Loss: 0.9388\nTest Loss: 1.0181\nEpoch 7/10\nTrain Loss: 0.9432\nTest Loss: 1.0219\nEpoch 8/10\nTrain Loss: 0.9468\nTest Loss: 1.0193\nEpoch 9/10\nTrain Loss: 0.9441\nTest Loss: 1.0148\nEpoch 10/10\nTrain Loss: 0.9391\nTest Loss: 1.0142\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport networkx as nx\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.utils import from_networkx\nfrom torch_scatter import scatter_add  # used for message aggregation in diffusion\n\n############################################\n# DATASET LOADER\n############################################\nclass TwitterTennisDatasetLoader:\n    def __init__(self, event_id=\"rg17\", N=1000, feature_mode=\"encoded\", target_offset=1, root=\"./data\"):\n        self.event_id = event_id\n        self.N = N\n        self.feature_mode = feature_mode\n        self.target_offset = target_offset\n        self.root = root\n        self.data_path = os.path.join(root, f\"{event_id}.csv\")\n\n        if not os.path.exists(self.data_path):\n            self.download_dataset()\n\n        self.dataset = self.process_dataset()\n\n    def download_dataset(self):\n        os.makedirs(self.root, exist_ok=True)\n        print(f\"Downloading Twitter Tennis dataset for event {self.event_id}...\")\n\n        dummy_data = {\n            \"source\": np.random.randint(0, self.N, size=5000),\n            \"target\": np.random.randint(0, self.N, size=5000),\n            \"interaction\": np.random.choice([\"mention\", \"retweet\", \"reply\"], size=5000),\n            \"timestamp\": np.random.randint(1609459200, 1612137600, size=5000),\n            \"feature1\": np.random.randn(5000),\n            \"feature2\": np.random.randn(5000),\n        }\n\n        df = pd.DataFrame(dummy_data)\n        df.to_csv(self.data_path, index=False)\n        print(f\"Dataset saved at {self.data_path}\")\n\n    def process_dataset(self):\n        print(\"Processing dataset...\")\n\n        df = pd.read_csv(self.data_path)\n        interaction_map = {\"mention\": 0, \"retweet\": 1, \"reply\": 2}\n        df[\"interaction\"] = df[\"interaction\"].map(interaction_map)\n\n        # Create a directed graph from the data\n        G = nx.DiGraph()\n        for _, row in df.iterrows():\n            G.add_edge(row[\"source\"], row[\"target\"], weight=row[\"interaction\"])\n\n        # Assign random node features and a random time attribute to each node\n        for node in G.nodes:\n            G.nodes[node][\"feature1\"] = np.random.randn()\n            G.nodes[node][\"feature2\"] = np.random.randn()\n            # Random time value between 0 and 1 (could represent a normalized timestamp)\n            G.nodes[node][\"time\"] = np.random.rand()\n\n        # Convert the NetworkX graph to a PyG Data object\n        data = from_networkx(G)\n\n        # Create node-level target values (one per node)\n        data.y = torch.tensor(np.random.randn(len(G.nodes)), dtype=torch.float32)\n\n        # Correctly stack node features: each node gets a feature vector [feature1, feature2]\n        features = [[G.nodes[node][\"feature1\"], G.nodes[node][\"feature2\"]] for node in G.nodes]\n        data.x = torch.tensor(features, dtype=torch.float32)\n\n        # Also add the \"time\" attribute as a node-level feature (shape: [num_nodes, 1])\n        times = [G.nodes[node][\"time\"] for node in G.nodes]\n        data.time = torch.tensor(times, dtype=torch.float32).unsqueeze(1)\n\n        print(\"Dataset processing complete.\")\n        return data\n\n    def get_dataset(self):\n        return self.dataset\n\n############################################\n# MODEL: TIME AWARE RANDOM WALK DIFFUSION\n############################################\nclass TimeAwareRandomWalkDiffusion(nn.Module):\n    def __init__(self, dim_node_features, dim_target, readout_class, config):\n        \"\"\"\n        dim_node_features: Dimension of input node features.\n        dim_target: Dimension of target output.\n        readout_class: A predictor module (e.g., an MLP) that produces the final output.\n        config: Dictionary of configuration parameters. Expected keys:\n                - alpha: weighting factor between original and diffused features.\n                - num_steps: number of diffusion steps.\n        \"\"\"\n        super().__init__()\n        self.dim_node_features = dim_node_features\n        self.dim_target = dim_target\n        self.alpha = config.get(\"alpha\", 0.2)\n        self.num_steps = config.get(\"num_steps\", 3)\n        # Linear layer to project diffused features to target dimension.\n        self.lin = nn.Linear(dim_node_features, dim_target)\n        # Readout/predictor module (e.g., an MLP)\n        self.readout = readout_class(dim_target, 1, dim_target, config)\n        self.relu = nn.ReLU()\n\n    def forward(self, snapshot, prev_state=None):\n        \"\"\"\n        snapshot: a Data object containing x (features), edge_index, and time.\n        \"\"\"\n        x, edge_index = snapshot.x, snapshot.edge_index\n        # Get node times; shape: [num_nodes, 1]\n        time = snapshot.time if hasattr(snapshot, \"time\") else None\n        num_nodes = x.size(0)\n\n        # Compute edge weights based on time differences.\n        if time is not None:\n            # For each edge, compute: weight = exp(-|time[source] - time[target]|)\n            src_time = time[edge_index[0]]  # shape: [num_edges, 1]\n            tgt_time = time[edge_index[1]]  # shape: [num_edges, 1]\n            weights = torch.exp(-torch.abs(src_time - tgt_time)).squeeze()  # shape: [num_edges]\n        else:\n            weights = torch.ones(edge_index.size(1), device=x.device)\n\n        # Perform diffusion over multiple steps.\n        diffused = x.clone()\n        for _ in range(self.num_steps):\n            # Aggregate messages from neighbors.\n            # For each edge from node j (source) to node i (target):\n            #   message = weight * diffused[j]\n            aggr = scatter_add(weights.unsqueeze(1) * diffused[edge_index[0]], edge_index[1], dim=0, dim_size=num_nodes)\n            # Also compute the sum of weights per target to normalize.\n            norm = scatter_add(weights.unsqueeze(1), edge_index[1], dim=0, dim_size=num_nodes)\n            # Avoid division by zero.\n            norm[norm == 0] = 1\n            # Combine original features and aggregated messages.\n            diffused = self.alpha * diffused + (1 - self.alpha) * (aggr / norm)\n\n        # Project diffused features and apply activation.\n        h = self.relu(self.lin(diffused))\n        # Get final output via the readout/predictor module.\n        out, _ = self.readout(h, None)\n        return out, h\n\n############################################\n# MLP PREDICTOR (used as the readout)\n############################################\nclass MLP_Predictor(nn.Module):\n    def __init__(self, dim_node_features, dim_edge_features, dim_target, config):\n        super().__init__()\n        self.fc1 = nn.Linear(dim_node_features, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, dim_target)\n        self.relu = nn.ReLU()\n\n    def forward(self, h, _):\n        h = self.relu(self.fc1(h))\n        h = self.relu(self.fc2(h))\n        return self.fc3(h), None\n\n############################################\n# SETUP DATA, MODEL, TRAINING, TESTING\n############################################\n\n# Load dataset\ndataset_loader = TwitterTennisDatasetLoader()\ndataset = dataset_loader.get_dataset()\n\n# Split dataset into train and test sets (node-level split)\ntrain_size = int(0.8 * dataset.num_nodes)\ntrain_mask = torch.zeros(dataset.num_nodes, dtype=torch.bool)\ntrain_mask[torch.randperm(dataset.num_nodes)[:train_size]] = True\ntest_mask = ~train_mask\n\n# Create Data objects with masks (for node-level training/testing)\ntrain_data = Data(x=dataset.x, edge_index=dataset.edge_index, y=dataset.y, time=dataset.time, train_mask=train_mask)\ntest_data = Data(x=dataset.x, edge_index=dataset.edge_index, y=dataset.y, time=dataset.time, test_mask=test_mask)\n\ntrain_loader = DataLoader([train_data], batch_size=1)\ntest_loader = DataLoader([test_data], batch_size=1)\n\n# Configuration for the model (alpha and num_steps for diffusion, etc.)\nconfig = {\"alpha\": 0.2, \"num_steps\": 3}\n\n# Initialize Model\n# Note: dataset.num_node_features is 2 (feature1 and feature2)\nmodel = TimeAwareRandomWalkDiffusion(dataset.num_node_features, 1, MLP_Predictor, config)\noptimizer = optim.Adam(model.parameters(), lr=0.01)\ncriterion = nn.MSELoss()\n\ndef train():\n    model.train()\n    total_loss = 0\n    for snapshot in train_loader:\n        optimizer.zero_grad()\n        out, _ = model(snapshot)\n        loss = criterion(out[snapshot.train_mask].view(-1), snapshot.y[snapshot.train_mask])\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Train Loss: {total_loss:.4f}\")\n\ndef test():\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for snapshot in test_loader:\n            out, _ = model(snapshot)\n            loss = criterion(out[snapshot.test_mask].view(-1), snapshot.y[snapshot.test_mask])\n            total_loss += loss.item()\n    print(f\"Test Loss: {total_loss:.4f}\")\n\n# Train and Test Model\nfor epoch in range(10):\n    print(f\"Epoch {epoch+1}/10\")\n    train()\n    test()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T16:51:37.082829Z","iopub.execute_input":"2025-02-17T16:51:37.083197Z","iopub.status.idle":"2025-02-17T16:51:37.477906Z","shell.execute_reply.started":"2025-02-17T16:51:37.083171Z","shell.execute_reply":"2025-02-17T16:51:37.477084Z"}},"outputs":[{"name":"stdout","text":"Processing dataset...\nDataset processing complete.\nEpoch 1/10\nTrain Loss: 1.0079\nTest Loss: 0.9605\nEpoch 2/10\nTrain Loss: 0.9981\nTest Loss: 0.9600\nEpoch 3/10\nTrain Loss: 0.9971\nTest Loss: 0.9593\nEpoch 4/10\nTrain Loss: 0.9920\nTest Loss: 0.9628\nEpoch 5/10\nTrain Loss: 0.9907\nTest Loss: 0.9678\nEpoch 6/10\nTrain Loss: 0.9918\nTest Loss: 0.9707\nEpoch 7/10\nTrain Loss: 0.9928\nTest Loss: 0.9702\nEpoch 8/10\nTrain Loss: 0.9924\nTest Loss: 0.9676\nEpoch 9/10\nTrain Loss: 0.9912\nTest Loss: 0.9646\nEpoch 10/10\nTrain Loss: 0.9903\nTest Loss: 0.9624\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport networkx as nx\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.utils import from_networkx\nfrom torch_scatter import scatter_add  # For aggregating messages\n\n############################################\n# DATASET LOADER\n############################################\nclass TwitterTennisDatasetLoader:\n    def __init__(self, event_id=\"rg17\", N=1000, feature_mode=\"encoded\", target_offset=1, root=\"./data\"):\n        self.event_id = event_id\n        self.N = N\n        self.feature_mode = feature_mode\n        self.target_offset = target_offset\n        self.root = root\n        self.data_path = os.path.join(root, f\"{event_id}.csv\")\n\n        if not os.path.exists(self.data_path):\n            self.download_dataset()\n\n        self.dataset = self.process_dataset()\n\n    def download_dataset(self):\n        os.makedirs(self.root, exist_ok=True)\n        print(f\"Downloading Twitter Tennis dataset for event {self.event_id}...\")\n\n        dummy_data = {\n            \"source\": np.random.randint(0, self.N, size=5000),\n            \"target\": np.random.randint(0, self.N, size=5000),\n            \"interaction\": np.random.choice([\"mention\", \"retweet\", \"reply\"], size=5000),\n            \"timestamp\": np.random.randint(1609459200, 1612137600, size=5000),\n            \"feature1\": np.random.randn(5000),\n            \"feature2\": np.random.randn(5000),\n        }\n\n        df = pd.DataFrame(dummy_data)\n        df.to_csv(self.data_path, index=False)\n        print(f\"Dataset saved at {self.data_path}\")\n\n    def process_dataset(self):\n        print(\"Processing dataset...\")\n\n        df = pd.read_csv(self.data_path)\n        interaction_map = {\"mention\": 0, \"retweet\": 1, \"reply\": 2}\n        df[\"interaction\"] = df[\"interaction\"].map(interaction_map)\n\n        # Create a directed graph from the data\n        G = nx.DiGraph()\n        for _, row in df.iterrows():\n            G.add_edge(row[\"source\"], row[\"target\"], weight=row[\"interaction\"])\n\n        # Assign random node features and a random \"time\" attribute to each node\n        for node in G.nodes:\n            G.nodes[node][\"feature1\"] = np.random.randn()\n            G.nodes[node][\"feature2\"] = np.random.randn()\n            # Random time value between 0 and 1 (could be a normalized timestamp)\n            G.nodes[node][\"time\"] = np.random.rand()\n\n        # Convert the NetworkX graph to a PyG Data object\n        data = from_networkx(G)\n\n        # Create node-level target values (one per node)\n        data.y = torch.tensor(np.random.randn(len(G.nodes)), dtype=torch.float32)\n\n        # Correctly stack node features: each node gets a feature vector [feature1, feature2]\n        features = [[G.nodes[node][\"feature1\"], G.nodes[node][\"feature2\"]] for node in G.nodes]\n        data.x = torch.tensor(features, dtype=torch.float32)\n\n        # Add the \"time\" attribute as a node-level feature (shape: [num_nodes, 1])\n        times = [G.nodes[node][\"time\"] for node in G.nodes]\n        data.time = torch.tensor(times, dtype=torch.float32).unsqueeze(1)\n\n        print(\"Dataset processing complete.\")\n        return data\n\n    def get_dataset(self):\n        return self.dataset\n\n############################################\n# MODEL: TiaRaGNNModel (Time-Aware Random Walk GNN)\n############################################\nclass TiaRaGNNModel(nn.Module):\n    def __init__(self, dim_node_features, dim_target, readout_class, config):\n        \"\"\"\n        TiaRaGNNModel: A Time-Aware Random Walk Graph Neural Network Model.\n        It performs a diffusion of node features weighted by the time differences\n        between nodes, and then applies a linear projection followed by a readout.\n        \n        Parameters:\n          - dim_node_features: Dimension of input node features.\n          - dim_target: Dimension of target output.\n          - readout_class: A predictor module (e.g., an MLP) that produces the final output.\n          - config: Dictionary of configuration parameters, e.g.,\n                    \"alpha\": weighting factor between original and diffused features.\n                    \"num_steps\": number of diffusion steps.\n        \"\"\"\n        super().__init__()\n        self.dim_node_features = dim_node_features\n        self.dim_target = dim_target\n        self.alpha = config.get(\"alpha\", 0.2)\n        self.num_steps = config.get(\"num_steps\", 3)\n        # Linear layer to project diffused features to target dimension.\n        self.lin = nn.Linear(dim_node_features, dim_target)\n        # Readout/predictor module (e.g., an MLP)\n        self.readout = readout_class(dim_target, 1, dim_target, config)\n        self.relu = nn.ReLU()\n\n    def forward(self, snapshot, prev_state=None):\n        \"\"\"\n        snapshot: a Data object containing x (features), edge_index, and time.\n        \"\"\"\n        x, edge_index = snapshot.x, snapshot.edge_index\n        # Get node times; shape: [num_nodes, 1]\n        time = snapshot.time if hasattr(snapshot, \"time\") else None\n        num_nodes = x.size(0)\n\n        # Compute edge weights based on time differences.\n        if time is not None:\n            # For each edge, compute: weight = exp(-|time[source] - time[target]|)\n            src_time = time[edge_index[0]]  # shape: [num_edges, 1]\n            tgt_time = time[edge_index[1]]  # shape: [num_edges, 1]\n            weights = torch.exp(-torch.abs(src_time - tgt_time)).squeeze()  # shape: [num_edges]\n        else:\n            weights = torch.ones(edge_index.size(1), device=x.device)\n\n        # Perform diffusion over multiple steps.\n        diffused = x.clone()\n        for _ in range(self.num_steps):\n            # Aggregate messages from neighbors:\n            aggr = scatter_add(weights.unsqueeze(1) * diffused[edge_index[0]], edge_index[1],\n                                 dim=0, dim_size=num_nodes)\n            # Compute the sum of weights per target to normalize.\n            norm = scatter_add(weights.unsqueeze(1), edge_index[1], dim=0, dim_size=num_nodes)\n            # Avoid division by zero.\n            norm[norm == 0] = 1\n            # Combine original features and aggregated messages.\n            diffused = self.alpha * diffused + (1 - self.alpha) * (aggr / norm)\n\n        # Project diffused features and apply activation.\n        h = self.relu(self.lin(diffused))\n        # Get final output via the readout/predictor module.\n        out, _ = self.readout(h, None)\n        return out, h\n\n############################################\n# MLP PREDICTOR (used as the readout)\n############################################\nclass MLP_Predictor(nn.Module):\n    def __init__(self, dim_node_features, dim_edge_features, dim_target, config):\n        super().__init__()\n        self.fc1 = nn.Linear(dim_node_features, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, dim_target)\n        self.relu = nn.ReLU()\n\n    def forward(self, h, _):\n        h = self.relu(self.fc1(h))\n        h = self.relu(self.fc2(h))\n        return self.fc3(h), None\n\n############################################\n# SETUP DATA, MODEL, TRAINING, TESTING\n############################################\n\n# Load dataset\ndataset_loader = TwitterTennisDatasetLoader()\ndataset = dataset_loader.get_dataset()\n\n# Split dataset into train and test sets (node-level split)\ntrain_size = int(0.8 * dataset.num_nodes)\ntrain_mask = torch.zeros(dataset.num_nodes, dtype=torch.bool)\ntrain_mask[torch.randperm(dataset.num_nodes)[:train_size]] = True\ntest_mask = ~train_mask\n\n# Create Data objects with masks (for node-level training/testing)\ntrain_data = Data(x=dataset.x, edge_index=dataset.edge_index, y=dataset.y,\n                  time=dataset.time, train_mask=train_mask)\ntest_data = Data(x=dataset.x, edge_index=dataset.edge_index, y=dataset.y,\n                 time=dataset.time, test_mask=test_mask)\n\ntrain_loader = DataLoader([train_data], batch_size=1)\ntest_loader = DataLoader([test_data], batch_size=1)\n\n# Configuration for the model (alpha and num_steps for diffusion, etc.)\nconfig = {\"alpha\": 0.2, \"num_steps\": 3}\n\n# Initialize Model\n# Note: dataset.num_node_features is 2 (feature1 and feature2)\nmodel = TiaRaGNNModel(dataset.num_node_features, 1, MLP_Predictor, config)\noptimizer = optim.Adam(model.parameters(), lr=0.01)\ncriterion = nn.MSELoss()\n\ndef train():\n    model.train()\n    total_loss = 0\n    for snapshot in train_loader:\n        optimizer.zero_grad()\n        out, _ = model(snapshot)\n        loss = criterion(out[snapshot.train_mask].view(-1), snapshot.y[snapshot.train_mask])\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Train Loss: {total_loss:.4f}\")\n\ndef test():\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for snapshot in test_loader:\n            out, _ = model(snapshot)\n            loss = criterion(out[snapshot.test_mask].view(-1), snapshot.y[snapshot.test_mask])\n            total_loss += loss.item()\n    print(f\"Test Loss: {total_loss:.4f}\")\n\n# Train and Test Model\nfor epoch in range(10):\n    print(f\"Epoch {epoch+1}/10\")\n    train()\n    test()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T16:56:14.105981Z","iopub.execute_input":"2025-02-17T16:56:14.106327Z","iopub.status.idle":"2025-02-17T16:56:14.484116Z","shell.execute_reply.started":"2025-02-17T16:56:14.106305Z","shell.execute_reply":"2025-02-17T16:56:14.483270Z"}},"outputs":[{"name":"stdout","text":"Processing dataset...\nDataset processing complete.\nEpoch 1/10\nTrain Loss: 0.9593\nTest Loss: 1.0365\nEpoch 2/10\nTrain Loss: 0.9577\nTest Loss: 1.0363\nEpoch 3/10\nTrain Loss: 0.9576\nTest Loss: 1.0300\nEpoch 4/10\nTrain Loss: 0.9561\nTest Loss: 1.0247\nEpoch 5/10\nTrain Loss: 0.9557\nTest Loss: 1.0217\nEpoch 6/10\nTrain Loss: 0.9561\nTest Loss: 1.0204\nEpoch 7/10\nTrain Loss: 0.9566\nTest Loss: 1.0204\nEpoch 8/10\nTrain Loss: 0.9566\nTest Loss: 1.0211\nEpoch 9/10\nTrain Loss: 0.9563\nTest Loss: 1.0226\nEpoch 10/10\nTrain Loss: 0.9559\nTest Loss: 1.0245\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport networkx as nx\nfrom torch_geometric.data import Data, DataLoader\nfrom torch_geometric.utils import from_networkx\nfrom torch_scatter import scatter_add  # For message aggregation\n\n############################################\n# DATASET LOADER\n############################################\nclass TwitterTennisDatasetLoader:\n    def __init__(self, event_id=\"rg17\", N=1000, feature_mode=\"encoded\", target_offset=1, root=\"./data\"):\n        self.event_id = event_id\n        self.N = N\n        self.feature_mode = feature_mode\n        self.target_offset = target_offset\n        self.root = root\n        self.data_path = os.path.join(root, f\"{event_id}.csv\")\n\n        if not os.path.exists(self.data_path):\n            self.download_dataset()\n\n        self.dataset = self.process_dataset()\n\n    def download_dataset(self):\n        os.makedirs(self.root, exist_ok=True)\n        print(f\"Downloading Twitter Tennis dataset for event {self.event_id}...\")\n\n        dummy_data = {\n            \"source\": np.random.randint(0, self.N, size=5000),\n            \"target\": np.random.randint(0, self.N, size=5000),\n            \"interaction\": np.random.choice([\"mention\", \"retweet\", \"reply\"], size=5000),\n            \"timestamp\": np.random.randint(1609459200, 1612137600, size=5000),\n            \"feature1\": np.random.randn(5000),\n            \"feature2\": np.random.randn(5000),\n        }\n\n        df = pd.DataFrame(dummy_data)\n        df.to_csv(self.data_path, index=False)\n        print(f\"Dataset saved at {self.data_path}\")\n\n    def process_dataset(self):\n        print(\"Processing dataset...\")\n\n        df = pd.read_csv(self.data_path)\n        interaction_map = {\"mention\": 0, \"retweet\": 1, \"reply\": 2}\n        df[\"interaction\"] = df[\"interaction\"].map(interaction_map)\n\n        # Create a directed graph from the data\n        G = nx.DiGraph()\n        for _, row in df.iterrows():\n            G.add_edge(row[\"source\"], row[\"target\"], weight=row[\"interaction\"])\n\n        # Assign random node features and a random \"time\" attribute to each node\n        for node in G.nodes:\n            G.nodes[node][\"feature1\"] = np.random.randn()\n            G.nodes[node][\"feature2\"] = np.random.randn()\n            # Random time value between 0 and 1 (a normalized timestamp)\n            G.nodes[node][\"time\"] = np.random.rand()\n\n        # Convert the NetworkX graph to a PyG Data object\n        data = from_networkx(G)\n\n        # Create node-level target values (one per node)\n        data.y = torch.tensor(np.random.randn(len(G.nodes)), dtype=torch.float32)\n\n        # Correctly stack node features: each node gets a feature vector [feature1, feature2]\n        features = [[G.nodes[node][\"feature1\"], G.nodes[node][\"feature2\"]] for node in G.nodes]\n        data.x = torch.tensor(features, dtype=torch.float32)\n\n        # Add the \"time\" attribute as a node-level feature (shape: [num_nodes, 1])\n        times = [G.nodes[node][\"time\"] for node in G.nodes]\n        data.time = torch.tensor(times, dtype=torch.float32).unsqueeze(1)\n\n        print(\"Dataset processing complete.\")\n        return data\n\n    def get_dataset(self):\n        return self.dataset\n\n############################################\n# MODEL: DynamicGEMModel\n############################################\nclass DynamicGEMModel(nn.Module):\n    def __init__(self, dim_node_features, dim_target, readout_class, config):\n        \"\"\"\n        DynamicGEMModel: A dynamic graph embedding model that evolves node features\n        over multiple diffusion steps. It uses a GRU cell to update node embeddings based on\n        time-aware neighborhood aggregation.\n        \n        Parameters:\n          - dim_node_features: Dimension of input node features.\n          - dim_target: Dimension of target output.\n          - readout_class: A predictor module (e.g., an MLP) that produces the final output.\n          - config: Dictionary of configuration parameters, e.g.,\n                    \"alpha\": weighting factor between current features and aggregated messages.\n                    \"num_steps\": number of diffusion (or evolution) steps.\n        \"\"\"\n        super().__init__()\n        self.alpha = config.get(\"alpha\", 0.2)\n        self.num_steps = config.get(\"num_steps\", 3)\n        # GRUCell for dynamic update (input and hidden size equal to dim_node_features)\n        self.gru = nn.GRUCell(dim_node_features, dim_node_features)\n        # Linear projection after evolution\n        self.lin = nn.Linear(dim_node_features, dim_target)\n        # Readout/predictor module (e.g., an MLP)\n        self.readout = readout_class(dim_target, 1, dim_target, config)\n        self.relu = nn.ReLU()\n\n    def forward(self, snapshot, prev_state=None):\n        \"\"\"\n        snapshot: a Data object containing x (node features), edge_index, and time.\n        \"\"\"\n        x, edge_index = snapshot.x, snapshot.edge_index\n        time = snapshot.time if hasattr(snapshot, \"time\") else None\n        num_nodes = x.size(0)\n\n        # Compute edge weights based on time differences\n        if time is not None:\n            src_time = time[edge_index[0]]  # shape: [num_edges, 1]\n            tgt_time = time[edge_index[1]]  # shape: [num_edges, 1]\n            weights = torch.exp(-torch.abs(src_time - tgt_time)).squeeze()  # shape: [num_edges]\n        else:\n            weights = torch.ones(edge_index.size(1), device=x.device)\n\n        # Initialize hidden state with node features\n        h = x\n\n        # Dynamic evolution: update node embeddings using GRUCell with aggregated messages.\n        for _ in range(self.num_steps):\n            # Aggregate messages from neighbors using time-aware weights.\n            aggr = scatter_add(weights.unsqueeze(1) * h[edge_index[0]], edge_index[1],\n                                 dim=0, dim_size=num_nodes)\n            # Compute normalization factor (sum of weights per node)\n            norm = scatter_add(weights.unsqueeze(1), edge_index[1], dim=0, dim_size=num_nodes)\n            norm[norm == 0] = 1  # avoid division by zero\n            message = aggr / norm\n            # Update hidden state using the GRU cell.\n            h = self.gru(message, h)\n\n        # Project the dynamically evolved embeddings and apply activation.\n        h_proj = self.relu(self.lin(h))\n        # Final output from the readout module.\n        out, _ = self.readout(h_proj, None)\n        return out, h\n\n############################################\n# MLP PREDICTOR (used as the readout)\n############################################\nclass MLP_Predictor(nn.Module):\n    def __init__(self, dim_node_features, dim_edge_features, dim_target, config):\n        super().__init__()\n        self.fc1 = nn.Linear(dim_node_features, 64)\n        self.fc2 = nn.Linear(64, 32)\n        self.fc3 = nn.Linear(32, dim_target)\n        self.relu = nn.ReLU()\n\n    def forward(self, h, _):\n        h = self.relu(self.fc1(h))\n        h = self.relu(self.fc2(h))\n        return self.fc3(h), None\n\n############################################\n# SETUP DATA, MODEL, TRAINING, TESTING\n############################################\n\n# Load dataset\ndataset_loader = TwitterTennisDatasetLoader()\ndataset = dataset_loader.get_dataset()\n\n# Split dataset into train and test sets (node-level split)\ntrain_size = int(0.8 * dataset.num_nodes)\ntrain_mask = torch.zeros(dataset.num_nodes, dtype=torch.bool)\ntrain_mask[torch.randperm(dataset.num_nodes)[:train_size]] = True\ntest_mask = ~train_mask\n\n# Create Data objects with masks (for node-level training/testing)\ntrain_data = Data(x=dataset.x, edge_index=dataset.edge_index, y=dataset.y,\n                  time=dataset.time, train_mask=train_mask)\ntest_data = Data(x=dataset.x, edge_index=dataset.edge_index, y=dataset.y,\n                 time=dataset.time, test_mask=test_mask)\n\ntrain_loader = DataLoader([train_data], batch_size=1)\ntest_loader = DataLoader([test_data], batch_size=1)\n\n# Configuration for the model\nconfig = {\"alpha\": 0.2, \"num_steps\": 3}\n\n# Initialize Model\n# Note: dataset.num_node_features is 2 (feature1 and feature2)\nmodel = DynamicGEMModel(dataset.num_node_features, 1, MLP_Predictor, config)\noptimizer = optim.Adam(model.parameters(), lr=0.01)\ncriterion = nn.MSELoss()\n\ndef train():\n    model.train()\n    total_loss = 0\n    for snapshot in train_loader:\n        optimizer.zero_grad()\n        out, _ = model(snapshot)\n        loss = criterion(out[snapshot.train_mask].view(-1), snapshot.y[snapshot.train_mask])\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Train Loss: {total_loss:.4f}\")\n\ndef test():\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for snapshot in test_loader:\n            out, _ = model(snapshot)\n            loss = criterion(out[snapshot.test_mask].view(-1), snapshot.y[snapshot.test_mask])\n            total_loss += loss.item()\n    print(f\"Test Loss: {total_loss:.4f}\")\n\n# Train and Test Model\nfor epoch in range(10):\n    print(f\"Epoch {epoch+1}/10\")\n    train()\n    test()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T16:57:15.147344Z","iopub.execute_input":"2025-02-17T16:57:15.147648Z","iopub.status.idle":"2025-02-17T16:57:15.568937Z","shell.execute_reply.started":"2025-02-17T16:57:15.147626Z","shell.execute_reply":"2025-02-17T16:57:15.568079Z"}},"outputs":[{"name":"stdout","text":"Processing dataset...\nDataset processing complete.\nEpoch 1/10\nTrain Loss: 1.0314\nTest Loss: 0.9769\nEpoch 2/10\nTrain Loss: 1.0487\nTest Loss: 0.9772\nEpoch 3/10\nTrain Loss: 1.0314\nTest Loss: 0.9945\nEpoch 4/10\nTrain Loss: 1.0330\nTest Loss: 1.0039\nEpoch 5/10\nTrain Loss: 1.0372\nTest Loss: 1.0003\nEpoch 6/10\nTrain Loss: 1.0356\nTest Loss: 0.9917\nEpoch 7/10\nTrain Loss: 1.0321\nTest Loss: 0.9839\nEpoch 8/10\nTrain Loss: 1.0301\nTest Loss: 0.9787\nEpoch 9/10\nTrain Loss: 1.0302\nTest Loss: 0.9760\nEpoch 10/10\nTrain Loss: 1.0314\nTest Loss: 0.9752\n","output_type":"stream"}],"execution_count":15}]}